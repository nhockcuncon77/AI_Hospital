# Architecture

The system is a **voice bot** that places outbound calls to the test line (805-439-8008), speaks as a patient (Minh Huynh, DOB July 14, 2001), and records both sides of the conversation for transcription and bug analysis.

**Flow:** Twilio places the call and, when the test line answers, connects the call to a **bidirectional Media Stream** over WebSockets to our server. We receive 8 kHz mulaw audio from the agent, run **Whisper** (STT) on buffered chunks, pass the text to a **patient LLM** (GPT-4o-mini) conditioned on a scenario (e.g. “schedule appointment”, “refill”), then synthesize the reply with **OpenAI TTS**, convert to 8 kHz mulaw, and send it back over the same WebSocket so Twilio plays it to the other party. We buffer about 2 seconds of inbound audio before each STT run to reduce overlap and allow natural turn-taking. When the stream ends we persist the full conversation to `transcripts/` as JSON. A separate script can run an LLM over those transcripts to produce a **bug report** (incorrect info, hallucinations, misunderstandings, awkward phrasing).

**Design choices:** (1) **Twilio + WebSocket** so we own the pipeline (STT/LLM/TTS) and can swap models or add logic without changing telephony. (2) **Scenario-based patient bot** so each call has a clear goal and first utterance, making it easy to cover scheduling, refills, hours, insurance, and edge cases like vague or “wrong number” openings. (3) **Batch STT every ~2 seconds** instead of streaming STT to keep the implementation simple and avoid talking over the agent; we trade a bit of latency for robustness. (4) **Single-threaded media loop** that processes when enough media has accumulated, so we don’t need separate reader/writer threads and avoid cross-thread WebSocket usage. (5) **Patient identity and DOB** are fixed in config so the agent can be tested on name/DOB handling consistently.
